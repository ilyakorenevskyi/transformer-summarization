{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/cnn_dailymail\",'3.0.0', trust_remote_code=True );"
   ],
   "id": "627da262ca2f8b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "12de87dd4193d5c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to count tokens in the dataset\n",
    "def count_tokens(example):\n",
    "    tokenized_text = tokenizer(example[\"article\"])\n",
    "    tokenized_summary = tokenizer(example[\"highlights\"])\n",
    "    return {\n",
    "        \"article_token_count\": len(tokenized_text[\"input_ids\"]),\n",
    "        \"summary_token_count\": len(tokenized_summary[\"input_ids\"]),\n",
    "    }\n",
    "\n",
    "token_counts = dataset.map(count_tokens, batched=False)\n"
   ],
   "id": "f052c6b636e9cefd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# Compute mean token count\n",
    "def compute_mean_token_count(token_counts, field_name):\n",
    "    total_token_count = sum(token_counts[field_name])\n",
    "    return total_token_count / len(token_counts)\n",
    "\n",
    "# Compute median token count\n",
    "def compute_median_token_count(token_counts, field_name):\n",
    "    token_counts_array = np.array(token_counts[field_name])\n",
    "    return np.median(token_counts_array)\n",
    "\n",
    "# Calculate and print mean and median token count for each split\n",
    "for split in token_counts:\n",
    "    mean_count_art = compute_mean_token_count(token_counts[split], \"article_token_count\")\n",
    "    mean_count_sum = compute_mean_token_count(token_counts[split], \"summary_token_count\")\n",
    "    median_count_art = compute_median_token_count(token_counts[split], \"article_token_count\")\n",
    "    median_count_sum = compute_median_token_count(token_counts[split], \"summary_token_count\")\n",
    "    print(f\"Mean token count for {split}: {mean_count_art} | {mean_count_sum}\")\n",
    "    print(f\"Median token count for {split}: {median_count_art} | {median_count_sum}\")"
   ],
   "id": "7358aaca19e08543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "article_token_counts = token_counts['train'][\"article_token_count\"]\n",
    "summary_token_counts = token_counts['train'][\"summary_token_count\"]\n",
    "\n",
    "# Create a histogram for article token counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(article_token_counts, bins=50, color='skyblue', edgecolor='black', range = (0, 2048))\n",
    "plt.xlabel(\"Article Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Article Token Counts\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a histogram for summary token counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(summary_token_counts, bins=50, color='salmon', edgecolor='black', range = (0, 128))\n",
    "plt.xlabel(\"Summary Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Summary Token Counts\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "52ff640d309efb67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prefix = \"summarize: \"\n",
    "postfix = \" </s>\"\n",
    "\n",
    "input_length = 512\n",
    "output_length = 70\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=input_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=output_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "id": "9e554e43da76dcf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenized_dataset = dataset.map(preprocess_function, batched=True)",
   "id": "8f8278cec3976cc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ],
   "id": "9a1a230565f00967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    #Use when overriding max output length\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}"
   ],
   "id": "1abe2fd597d5b5ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "num_epochs = 4\n",
    "batch_size = 8\n",
    "learining_rate = 2e-3\n",
    "num_beams = 4\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_t5_model_en_cnn\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learining_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_epochs,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=output_length,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "f9694dd31afc28f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train(resume_from_checkpoint=True)",
   "id": "4cfa1a432d7c9ad5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
