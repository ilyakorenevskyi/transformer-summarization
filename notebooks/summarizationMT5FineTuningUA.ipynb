{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:12.939643Z",
     "start_time": "2024-04-30T14:43:03.242507Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load Ukrainian dataset\n",
    "dataset = load_dataset(\"csebuetnlp/xlsum\", \"ukrainian\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:12.955163Z",
     "start_time": "2024-04-30T14:43:12.940645Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "12de87dd4193d5c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 43201\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 5399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "        num_rows: 5399\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:27.663170Z",
     "start_time": "2024-04-30T14:43:12.956163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens(dataset):\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenizer(dataset[\"text\"]) \n",
    "    tokenized_summary = tokenizer(dataset[\"summary\"])\n",
    "    \n",
    "    # Update the example with the token count\n",
    "    dataset[\"article_token_count\"] = len(tokenized_text[\"input_ids\"])\n",
    "    dataset[\"summary_token_count\"] = len(tokenized_summary[\"input_ids\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Apply the function and calculate the mean\n",
    "token_counts = dataset.map(count_tokens, batched=False)"
   ],
   "id": "1653612c7c921fbe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/43201 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c407002b4d546b9b8d7405e900e7d10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Apply the function and calculate the mean\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m token_counts \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcount_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py:868\u001B[0m, in \u001B[0;36mDatasetDict.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[0m\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    866\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    867\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[1;32m--> 868\u001B[0m     {\n\u001B[0;32m    869\u001B[0m         k: dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[0;32m    870\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m    871\u001B[0m             with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[0;32m    872\u001B[0m             with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[0;32m    873\u001B[0m             input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[0;32m    874\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[0;32m    875\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    876\u001B[0m             drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[0;32m    877\u001B[0m             remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[0;32m    878\u001B[0m             keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[0;32m    879\u001B[0m             load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[0;32m    880\u001B[0m             cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_names[k],\n\u001B[0;32m    881\u001B[0m             writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[0;32m    882\u001B[0m             features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m    883\u001B[0m             disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[0;32m    884\u001B[0m             fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[0;32m    885\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m    886\u001B[0m             desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[0;32m    887\u001B[0m         )\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    889\u001B[0m     }\n\u001B[0;32m    890\u001B[0m )\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py:869\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    866\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    867\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[0;32m    868\u001B[0m     {\n\u001B[1;32m--> 869\u001B[0m         k: \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    871\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    872\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    873\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m            \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    882\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    883\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    886\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    887\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    889\u001B[0m     }\n\u001B[0;32m    890\u001B[0m )\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:593\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    591\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    592\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 593\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    594\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:558\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    551\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    556\u001B[0m }\n\u001B[0;32m    557\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 558\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    559\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    560\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3105\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3100\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[0;32m   3101\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3102\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3103\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3104\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3105\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3106\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3107\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3457\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batched:\n\u001B[0;32m   3456\u001B[0m     _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m-> 3457\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[0;32m   3458\u001B[0m         example \u001B[38;5;241m=\u001B[39m apply_function_on_filtered_inputs(example, i, offset\u001B[38;5;241m=\u001B[39moffset)\n\u001B[0;32m   3459\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m update_data:\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2386\u001B[0m, in \u001B[0;36mDataset.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2384\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[0;32m   2385\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER\n\u001B[1;32m-> 2386\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pa_subtable \u001B[38;5;129;01min\u001B[39;00m table_iter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata, batch_size\u001B[38;5;241m=\u001B[39mbatch_size):\n\u001B[0;32m   2387\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(pa_subtable\u001B[38;5;241m.\u001B[39mnum_rows):\n\u001B[0;32m   2388\u001B[0m         pa_subtable_ex \u001B[38;5;241m=\u001B[39m pa_subtable\u001B[38;5;241m.\u001B[39mslice(i, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\Programming\\Projects\\transformerModels\\.venv\\lib\\site-packages\\datasets\\table.py:2342\u001B[0m, in \u001B[0;36mtable_iter\u001B[1;34m(table, batch_size, drop_last_batch)\u001B[0m\n\u001B[0;32m   2340\u001B[0m chunks_buffer_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   2341\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m table\u001B[38;5;241m.\u001B[39mto_reader(max_chunksize\u001B[38;5;241m=\u001B[39mbatch_size):\n\u001B[1;32m-> 2342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mlen\u001B[39;49m(chunk) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2343\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   2344\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m chunks_buffer_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(chunk) \u001B[38;5;241m<\u001B[39m batch_size:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:27.664168Z",
     "start_time": "2024-04-30T14:43:27.664168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "article_token_counts = token_counts['train'][\"article_token_count\"]\n",
    "summary_token_counts = token_counts['train'][\"summary_token_count\"]\n",
    "\n",
    "# Create a histogram for article token counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(article_token_counts, bins=50, color='skyblue', edgecolor='black', range = (0, 2048))\n",
    "plt.xlabel(\"Article Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Article Token Counts\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a histogram for summary token counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(summary_token_counts, bins=50, color='salmon', edgecolor='black', range = (0, 128))\n",
    "plt.xlabel(\"Summary Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Summary Token Counts\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "6c172390efe4ca73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to count tokens in the dataset\n",
    "def count_tokens(example):\n",
    "    tokenized_text = tokenizer(example[\"text\"])\n",
    "    tokenized_summary = tokenizer(example[\"summary\"])\n",
    "    return {\n",
    "        \"article_token_count\": len(tokenized_text[\"input_ids\"]),\n",
    "        \"summary_token_count\": len(tokenized_summary[\"input_ids\"]),\n",
    "    }\n",
    "\n",
    "# Apply the function to the dataset\n",
    "token_counts = dataset.map(count_tokens, batched=False)\n",
    "\n"
   ],
   "id": "f052c6b636e9cefd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to compute mean token count\n",
    "def compute_mean_token_count(token_counts, field_name):\n",
    "    total_token_count = sum(token_counts[field_name])\n",
    "    return total_token_count / len(token_counts)\n",
    "\n",
    "# Function to compute median token count\n",
    "def compute_median_token_count(token_counts, field_name):\n",
    "    token_counts_array = np.array(token_counts[field_name])\n",
    "    return np.median(token_counts_array)\n",
    "\n",
    "# Calculate and print mean and median token count for each split\n",
    "for split in token_counts:\n",
    "    mean_count_art = compute_mean_token_count(token_counts[split], \"article_token_count\")\n",
    "    mean_count_sum = compute_mean_token_count(token_counts[split], \"summary_token_count\")\n",
    "    median_count_art = compute_median_token_count(token_counts[split], \"article_token_count\")\n",
    "    median_count_sum = compute_median_token_count(token_counts[split], \"summary_token_count\")\n",
    "    print(f\"Mean token count for {split}: {mean_count_art} | {mean_count_sum}\")\n",
    "    print(f\"Median token count for {split}: {median_count_art} | {median_count_sum}\")"
   ],
   "id": "6a02cd358e811045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocessing function for the dataset\n",
    "prefix = \"summarize: \"\n",
    "postfix = \" </s>\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix +  doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    outputs = [doc + postfix for doc in examples[\"summary\"]]\n",
    "    labels = tokenizer(outputs, max_length=64, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "id": "9e554e43da76dcf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ],
   "id": "fa0caae5e7d30282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ],
   "id": "9a1a230565f00967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ],
   "id": "1abe2fd597d5b5ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:41.263471Z",
     "start_time": "2024-04-30T14:43:41.189313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_mt5_model_uk\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ],
   "id": "f9694dd31afc28f7",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", evaluation_results)"
   ],
   "id": "74a6c17091cc438f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:43:36.941342Z",
     "start_time": "2024-04-30T14:43:36.895856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "ca14d9a505859a09",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:39:18.109853Z",
     "start_time": "2024-04-30T14:43:44.247166Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(resume_from_checkpoint=True)",
   "id": "b16dc5be2e0d003e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43208' max='43208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43208/43208 1:55:29, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.869500</td>\n",
       "      <td>2.185976</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>40.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.703400</td>\n",
       "      <td>2.194303</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>38.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.574500</td>\n",
       "      <td>2.224762</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>38.816300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=43208, training_loss=0.5334326580395811, metrics={'train_runtime': 6929.4289, 'train_samples_per_second': 49.875, 'train_steps_per_second': 6.235, 'total_flos': 1.8270586080531456e+17, 'train_loss': 0.5334326580395811, 'epoch': 8.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
